전처리 되지 않은 데이터로는 분석이 불가능하다
우리가 해야할 일은 전처리한 데이터로 모델을 구성하는 것이다.

중요포인트!
1. 정제된 데이터를 x,y에 넣어주고
2. 최적의 weight를 구하기 위해서
3. 최소의 loss를 사용한다
4. 최소의 loss를 하기 위해 최적의 optimizer를 사용할 것이다.

구성!
1. 데이터
2. 모델구성
3. 컴파일, 훈련
4. 평가, 예측

batch_size의 default는 32이다.
batch_size가 데이터보다 클 경우 data의 사이즈의 최대로 잡아서 자동으로 돌려준다.

-> batch size는 한 번의 batch마다 주는 데이터 샘플의 size. 여기서 batch(보통 mini-batch라고 표현)는 나눠진 데이터 셋을 뜻하며 
-> iteration는 epoch를 나누어서 실행하는 횟수라고 생각하면 됨.​
-> 메모리의 한계와 속도 저하 때문에 대부분의 경우에는 한 번의 epoch에서 모든 데이터를 한꺼번에 집어넣을 수는 없습니다. 
그래서 데이터를 나누어서 주게 되는데 이때 몇 번 나누어서 주는가를 iteration, 각 iteration마다 주는 데이터 사이즈를 batch size라고 합니다.

accuracy에서 머신이 생각하는 결과값은 1과 0.999는 다르다고 생각한다. 
-> 따라서 선형회귀에서는 accuracy라는 평가 지표는 사용할 수 없다.
-> 분류모델 (ex 동전을 던졌을때 앞이냐 뒤냐?)
-> 예) 연예인이 길에서 나한테 프로포즈를 할 경우의수?
-> 1. 프로포즈를 한다 VS 안한다 - 결과는 한다 안한다 0 or 1

-> 개발자는 먼저 분류모델을 결정한다.

딥러닝에서는 회귀와 분류 두개를 사용!
-> regresser, classfier

Visual Studio Code 단축키
-> line에서 ctrl+/ - 라인주석
-> line에서 ctrl+c - 라인복사 ctrl+v 라인붙여넣기
-> line에서 ctrl+shift+del - 라인삭제

작업관리자 - gpu 1(그래픽카드) - copy - cuda 선택 - gpu를 사용하는 것을 볼 수 있다.

과제
1. mse(Mean Squard Error - 손실함수) 공부하기
-> 가장 일반적이고 직관적인 에러 지표. 에러를 제곱하여 평균을 계산하니, 값은 낮을수록 좋다.
-> 장점 - 지표 자체가 직관적이고 단순하다.
-> 단점 - 스케일에 의존적이다.
예를 들어, 삼성전자의 주가가 1000000원이고 네이버가 70000원일 때,
두 주가를 예측하는 각각 모델의 MSE 가 똑같이 5000이 나왔을 경우, 분명 동일한 에러율이 아님에도, 동일하게 보여짐.
에러를 제곱하기 때문에, 1미만의 에러는 더 작아지고, 그 이상의 에러는 더 커진다.
즉 값의 왜곡이 있음.

2. mae - 절대값의 합을 평균 계산
출처: https://dailyheumsi.tistory.com/167 [하나씩 점을 찍어 나가며]

3. RMSE(Root Mean Square Error)
-> RMSE란 말그대로 실험이나 관측에서 나타나는 오차(Error)를 제곱(Square)해서 평균(Mean)한 값의 제곱근(Root)을 뜻합니다.
-> RMSE = root{(e1^2 + e2^2 + … + en^2) / n}
-> 여기서 e1, e2는 참값과 관측값과의 차 입니다.
-> RMSE는 MSE의 루트값만 씌운것
출처: https://artwook.tistory.com/152

4. R2(결정계수)
-> R2값은 회귀 모델에서 예측의 적합도를 0과 1 사이의 값으로 계산한 것이다
-> 1은 예측이 완벽한 경우고, 0은 훈련 세트의 출력값인 y_train의 평균으로만 예측하는 모델의 경우이다

출처: https://ebbnflow.tistory.com/123

5. validation(검증)

6. 스칼라, 벡터, 행렬 텐서
-> 스칼라는 하나의 숫자를 의미합니다. - 1차원
-> 벡터는 숫자(스칼라)의 배열입니다. - 2차원
-> 행렬은 2차원의 배열입니다.
-> 텐서는 2차원 이상의 배열입니다.

출처: https://art28.github.io/blog/linear-algebra-1/

7. 행렬

8. mlp = multilayer perceptron